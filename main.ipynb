{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuJhUI2j973f"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from transformers import (\n",
    "    InstructBlipProcessor,\n",
    "    InstructBlipForConditionalGeneration,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xd-jbtaR-WCd",
    "outputId": "1507a3c3-7b99-4aec-f372-0473f0c6993d"
   },
   "outputs": [],
   "source": [
    "class ClassifierEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Toy contextual bandits classification task to demonstrate PR2L.\n",
    "\n",
    "  The agent receives an image from ImageNette and has to take one of 10 actions\n",
    "  corresponding to the class of the image. If correct, the agent receives +1. Else,\n",
    "  it receives 0 rewards.\n",
    "\n",
    "  Args:\n",
    "    num_imgs: int number of images from ImageNette to load. Default 1000.\n",
    "    download: bool whether or not to download the dataset. Only needs to be called\n",
    "      with True once. Default True\n",
    "    render_mode: Unused.\n",
    "  \"\"\"\n",
    "  def __init__(self, num_imgs=1000, download=True, render_mode=None, seed=0):\n",
    "    self.num_imgs = num_imgs\n",
    "\n",
    "    self.transform = transforms.Compose([transforms.CenterCrop(320)])\n",
    "\n",
    "    self.trainset = torchvision.datasets.Imagenette(root='./data', split=\"val\", size=\"320px\",\n",
    "                                                download=download, transform=self.transform)\n",
    "\n",
    "    self.classes = self.trainset.classes\n",
    "\n",
    "    self.images = []\n",
    "    self.labels = []\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    for i in np.random.permutation(len(self.trainset)):\n",
    "      img, label = self.trainset.__getitem__(i)\n",
    "      self.images.append(np.array(img))\n",
    "      self.labels.append(label)\n",
    "      if len(self.labels) >= num_imgs:\n",
    "        break\n",
    "\n",
    "    self.current_idx = 0\n",
    "\n",
    "    self.observation_space = gym.spaces.Box(low=0, high=255, shape=[320, 320, 3])\n",
    "    self.action_space = gym.spaces.Discrete(len(self.classes))\n",
    "\n",
    "    self.render_mode = render_mode\n",
    "\n",
    "  def reset(self, seed=0, options=None):\n",
    "    self.current_idx = np.random.choice(len(self.images))\n",
    "    return self.images[self.current_idx], {}\n",
    "\n",
    "  def step(self, action):\n",
    "    correct_action = self.labels[self.current_idx]\n",
    "\n",
    "    reward = 1 if action == correct_action else 0\n",
    "\n",
    "    info = {}\n",
    "    done = True\n",
    "    trunc = False\n",
    "\n",
    "    return self.images[0] * 0, reward, done, trunc, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write a wrapper that embeds images from the base environment with the VLM (using a provided prompt). In this case, the VLM is effectively part of the environment, with no gradients flowing through it. It simply acts as an encoder of images to yield promptable state representations. \n",
    "\n",
    "In principle, one could have the VLM be part of the policy and be updated with RL as well (akin to RT-2), but this is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAKFhnb9N277"
   },
   "outputs": [],
   "source": [
    "class VlmImgFeatureWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    env: gym Env to be wrapped.\n",
    "    vlm_model: transformers VLM being used to embed images\n",
    "    processor: transformers processor for the VLM\n",
    "    hidden_dim: int dimensionality of token embeddings for the VLM\n",
    "    prompt: str prompt given to the VLM with the image\n",
    "    last_n_layers: int number of layers of the transformer-based VLM whose outputed token\n",
    "        embeddings are included in the extracted promptable representation. Default 1\n",
    "        (use the token embeddings outputted by the final layer only)\n",
    "    use_encoder_embeds: bool, whether or not to use the token embeddings corresponding\n",
    "        to the prompt or input image. Default false (just use the embeddings of generated text)\n",
    "    skip_first: bool, whether or not to skip the first element of the hidden states object.\n",
    "        Depends on the VLM used. Default true.\n",
    "    move_to_cpu: bool, whether to move generated embeddings to CPU (as numpy array) or keep\n",
    "        it on same device as the VLM (as torch tensor). Default true.\n",
    "    verbose: bool, whether or not to print out generated text. Default false.\n",
    "    generate_kwargs: dict of generate keyword args used by VLM (see Huggingface documentation)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        vlm_model,\n",
    "        processor,\n",
    "        hidden_dim: int,\n",
    "        prompt: str,\n",
    "        img_shape,\n",
    "        last_n_layers=1,\n",
    "        use_encoder_embeds=False,\n",
    "        skip_first=True,\n",
    "        generate_kwargs=dict(\n",
    "            max_new_tokens=8,\n",
    "            min_new_tokens=8,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample=False,\n",
    "        ),\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.vlm_model = vlm_model\n",
    "        self.device = self.vlm_model.device\n",
    "        self.processor = processor\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prompt = prompt\n",
    "        self.last_n_layers = last_n_layers\n",
    "        self.use_encoder_embeds = use_encoder_embeds\n",
    "        self.generate_kwargs = generate_kwargs\n",
    "        self.skip_first = skip_first\n",
    "\n",
    "\n",
    "        self.observation_space = self._make_space(self.observation_space)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # modify obs\n",
    "        return self._process_obs(obs)\n",
    "\n",
    "    def _make_space(self, obs_space):\n",
    "        map = {}\n",
    "        new_shape = self._get_new_shape()\n",
    "        self.max_embs = new_shape[0]\n",
    "        print(\"Initializing VLM embedding observation space with...\")\n",
    "        print(f\"max_new_tokens = {self.generate_kwargs['max_new_tokens']}\")\n",
    "        print(f\"last_n_layers = {self.last_n_layers}\")\n",
    "        print(f\"Thus, setting final embedding observation shape to {new_shape}\")\n",
    "\n",
    "        new_v = gym.spaces.Box(low=-np.inf, high=np.inf, shape=new_shape)\n",
    "        map[\"seq\"] = new_v\n",
    "        map[\"seq_mask\"] = gym.spaces.Box(\n",
    "            low=np.ones(self.max_embs)*False, high=np.ones(self.max_embs)*True, dtype=bool\n",
    "        )\n",
    "        return gym.spaces.Dict(map)\n",
    "\n",
    "    def _process_obs(self, obs):\n",
    "        \"\"\"\n",
    "        Convert from an image observation to a dictionary with keys:\n",
    "            seq: Sequence of VLM representations corresponding to image of size\n",
    "                (max sequence length, token embed dim)\n",
    "            seq_mask: Mask of padding, vector of bools of length (max sequence length)\n",
    "        \"\"\"\n",
    "        map = {}\n",
    "        img = obs.copy()\n",
    "        if len(img.shape) < 4:\n",
    "            img = img.reshape([1] * (4 - len(img.shape)) + [*img.shape])\n",
    "        map[\"seq\"], map[\"seq_mask\"] = self._generate_embeds(img)\n",
    "        return map\n",
    "\n",
    "    def _get_new_shape(self):\n",
    "        \"\"\"\n",
    "        Gets the shape of the VLM representation sequence, for the purpose of creating\n",
    "        a suitable observation space\n",
    "        \"\"\"\n",
    "        seq, mask = self._generate_embeds(np.zeros(self.img_shape))\n",
    "        return seq.shape\n",
    "\n",
    "    def _generate_embeds(self, img):\n",
    "        \"\"\"\n",
    "        Pass prompt and image through VLM to yield hidden states, packaging it into\n",
    "        dictionary observation\n",
    "        \"\"\"\n",
    "        inputs = self.processor(images=img, text=self.prompt, return_tensors=\"pt\").to(\n",
    "            self.device\n",
    "        )\n",
    "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(\n",
    "            self.device, self.vlm_model.dtype\n",
    "        )\n",
    "        generated_ids = self.vlm_model.generate(**inputs, **self.generate_kwargs)\n",
    "        if \"hidden_states\" in generated_ids.keys():\n",
    "            hs = generated_ids[\"hidden_states\"]\n",
    "        elif \"decoder_hidden_states\" in generated_ids.keys():\n",
    "            assert not self.skip_first\n",
    "            hs = {\n",
    "                \"encoder\": generated_ids[\"encoder_hidden_states\"],\n",
    "                \"decoder\": generated_ids[\"decoder_hidden_states\"]\n",
    "                }\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return self._get_embeds(hs)\n",
    "\n",
    "\n",
    "    def _get_embeds(self, hs):\n",
    "        \"\"\"\n",
    "        Produces a single embedding tensor of shape:\n",
    "        [seq len, num hidden states, hidden state dims]\n",
    "\n",
    "        Args:\n",
    "            hs: tuple of tuple of tensors. Outer tuple has shape\n",
    "                # generated tokens (+ 1 if skip_first). Inner tuple\n",
    "                has shape (number of layers of self.vlm_model + 1).\n",
    "                Tensors in all layers other than\n",
    "            skip_first: bool (default True) on whether the first token\n",
    "                of hs should be skipped (ie if its the hidden states\n",
    "                for the prompt -- set to true for InstructBLIP/BLIP2)\n",
    "        Returns:\n",
    "            tuple of sequence of token embeds from VLM and corresponding\n",
    "                padding mask\n",
    "        \"\"\"\n",
    "        tokenwise_emb, mask = [], []\n",
    "\n",
    "        if not self.skip_first:\n",
    "            assert type(hs) is dict\n",
    "            # Used for T5-Flan InstructBLIP versions (as in this demo notebook)\n",
    "            embs = []\n",
    "            for i in range(len(hs[\"decoder\"])):\n",
    "                # [1, self.last_n_layers, token dim]\n",
    "                last_n_reps = torch.cat(hs[\"decoder\"][i][-self.last_n_layers:], dim = 1)\n",
    "                embs.append(last_n_reps)\n",
    "\n",
    "            # Create padding mask (True represents corresponding token is padding)\n",
    "            padding = [False] * len(embs) * self.last_n_layers\n",
    "            num_pad_tokens = self.generate_kwargs[\"max_new_tokens\"] - len(embs)\n",
    "            padding += [True] * self.last_n_layers * num_pad_tokens\n",
    "\n",
    "            if num_pad_tokens > 0:\n",
    "                embs += [torch.zeros(1, self.last_n_layers, self.hidden_dim)] * num_pad_tokens\n",
    "\n",
    "            # [number generated tokens, self.last_n_layers, token dim]\n",
    "            embs = torch.cat(embs, dim=0)\n",
    "\n",
    "            if self.use_encoder_embeds: # Whether or not to include representations from prompt/image\n",
    "                # [self.last_n_layers, num encoder tokens, token dim]\n",
    "                enc_embs = torch.cat(hs[\"encoder\"][-self.last_n_layers:], dim=0)\n",
    "                enc_embs = enc_embs.permute(1, 0, 2)\n",
    "                padding = [False] * len(enc_embs) * self.last_n_layers + padding\n",
    "\n",
    "                embs = torch.cat([enc_embs, embs], dim=0)\n",
    "\n",
    "            # [total tokens * self.last_n_layers, token dim]\n",
    "            embs = embs.reshape(-1, self.hidden_dim)\n",
    "\n",
    "            assert len(embs) == len(padding)\n",
    "\n",
    "            return embs.detach().cpu().numpy(), np.array(padding)\n",
    "\n",
    "        else:\n",
    "            # Used for Vicuna InstructBLIP versions\n",
    "            if self.use_encoder_embeds:\n",
    "                # shape: (# layers -> last n layers, # enc tokens, # hidden dims)\n",
    "                encoder_hs = torch.stack(hs[0], dim=0)[-self.last_n_layers :].detach()\n",
    "                # shape: (# enc tokens, # layers, # hidden dims)\n",
    "                encoder_hs = torch.transpose(encoder_hs, 0, 1)\n",
    "                # shape: (# enc tokens * # layers, # hidden dims)\n",
    "                encoder_hs = encoder_hs.reshape(-1, encoder_hs.shape[-1])\n",
    "                tokenwise_emb.append(encoder_hs.cpu().numpy())\n",
    "                mask += [False] * len(encoder_hs)\n",
    "            tokens = hs[1:]\n",
    "\n",
    "            for token in tokens:\n",
    "                for emb in token[-self.last_n_layers :]:\n",
    "                    final_token_emb = emb.detach().reshape(1, -1)\n",
    "                    if self.move_to_cpu:\n",
    "                        final_token_emb = final_token_emb.cpu().numpy()\n",
    "                    tokenwise_emb.append(final_token_emb)\n",
    "                    mask.append(False)\n",
    "\n",
    "            while len(mask) < self.max_embs:\n",
    "                emb = np.zeros([1, self.hidden_dim])\n",
    "                tokenwise_emb.append(emb)\n",
    "                mask.append(True)\n",
    "            return np.concatenate(tokenwise_emb), np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "epNZsLlb316H",
    "outputId": "4c2a773f-50c2-4067-c98d-5a560ebe6afb"
   },
   "outputs": [],
   "source": [
    "# Initialize the VLM\n",
    "VLM_DEVICE = \"cuda:7\"\n",
    "VLM_DTYPE = torch.float16\n",
    "\n",
    "vlm_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/instructblip-flan-t5-xl\", torch_dtype=VLM_DTYPE\n",
    "        ).to(VLM_DEVICE)\n",
    "processor = InstructBlipProcessor.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\"\n",
    ")\n",
    "vlm_dim = 2048\n",
    "skip_first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbxo7NBz52z7"
   },
   "outputs": [],
   "source": [
    "# Create env\n",
    "env = ClassifierEnv(download=False)\n",
    "\n",
    "# Wrap env with PR2L wrapper\n",
    "env = VlmImgFeatureWrapper(\n",
    "    env, \n",
    "    vlm_model=vlm_model, \n",
    "    processor=processor, \n",
    "    hidden_dim=vlm_dim, \n",
    "    img_shape=[320, 320, 3], \n",
    "    prompt=\"What is in this image?\", \n",
    "    # Answering this prompt correctly should yield good \n",
    "    # representations for getting linked to the proper category\n",
    "    last_n_layers=2,\n",
    "    use_encoder_embeds=True,\n",
    "    skip_first=skip_first\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
